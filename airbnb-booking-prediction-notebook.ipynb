{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import main libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# import ML libraries\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, dataset_type, age_mean=None):\n",
    "    \"\"\" This function takes a raw dataframe as input and clean it by:\n",
    "    - dropping duplicates\n",
    "    - reformatting some columns (dates, age etc.)\n",
    "    - extracting meaningful elements from datetimes\n",
    "    - dropping pointless columns or the ones that cause problems in the next steps\n",
    "    - dropping rows with missing values in important columns \n",
    "    \n",
    "    NB : If age_mean = None, then age_mean = df.age.mean()\n",
    "    \"\"\"\n",
    "\n",
    "    col_id = df.id\n",
    "    columns_to_remove = [ \n",
    "        'id', \n",
    "        'first_affiliate_tracked', \n",
    "        'date_first_booking'\n",
    "    ]\n",
    "\n",
    "    df.drop(columns_to_remove, axis=1, inplace=True)\n",
    "    \n",
    "    df.drop_duplicates(inplace=True) # drop duplicates in the dataset\n",
    "    \n",
    "    \n",
    "    df['timestamp_first_active'] = df['timestamp_first_active'].apply(lambda x: datetime.strptime(str(x), '%Y%m%d%H%M%S')) # convert timestamps to datetimes\n",
    "    df['date_account_created'] = pd.to_datetime(df.date_account_created) #convert object dates to datetimes\n",
    "    df['days_to_first_active'] = (pd.to_datetime(df.date_account_created) - df.timestamp_first_active).astype('timedelta64[D]').astype(int)\n",
    "    \n",
    "    df['year_first_active'] = df['timestamp_first_active'].dt.year.astype(str) # extract year timestamp_first_active\n",
    "    df['month_first_active'] = df['timestamp_first_active'].dt.month.astype(str) # extract month timestamp_first_active\n",
    "    df.drop(['timestamp_first_active'], axis=1, inplace=True)\n",
    "    \n",
    "    df['year_creation'] = df['date_account_created'].dt.year.astype(str) # extract year from date_account_created\n",
    "    df['month_creation'] = df['date_account_created'].dt.month.astype(str) # extract month date_account_created\n",
    "    df.drop(['date_account_created'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    dummies_list = [\n",
    "        'gender',\n",
    "        'signup_method',\n",
    "        'signup_flow',\n",
    "        'language',\n",
    "        'signup_app',\n",
    "        'year_creation',\n",
    "        'month_creation',\n",
    "        'year_first_active',\n",
    "        'month_first_active',\n",
    "        'first_device_type',\n",
    "        'first_browser',\n",
    "        'affiliate_channel',\n",
    "        'affiliate_provider'\n",
    "    ]\n",
    "    \n",
    "    dummy_df = pd.get_dummies(df[dummies_list])\n",
    "    df = df.drop(dummies_list, axis=1)\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    \n",
    "\n",
    "    if dataset_type == 'train':\n",
    "        cols = list(df)\n",
    "        cols.append(cols.pop(cols.index('country_destination')))\n",
    "        df = df[cols]\n",
    "    \n",
    "\n",
    "    \n",
    "    return df, col_id\n",
    "\n",
    "\n",
    "def get_class_weights(X):\n",
    "    \"\"\" This function takes a dataframe with multiple classes and labels as input \n",
    "    and computes the weights of the classes.\"\"\"\n",
    "    n_classes = X.country_destination.nunique()\n",
    "    weights = X.shape[0]/(X.country_destination.value_counts())\n",
    "    keys = weights.index.tolist()\n",
    "    values = weights.tolist()\n",
    "    cl_weights = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    return cl_weights\n",
    "\n",
    "\n",
    "def merge_sessions(df, session_set, keep_all=False):\n",
    "    # Clean the sessions set\n",
    "    session_set.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Checking if the missing values in action_type correspond to missing values in action_detail and if so, Mark 'unknown' as NaN\n",
    "    if (session_set.action_type.isna() == session_set.action_detail.isna()).sum() == len(session_set):\n",
    "        session_set.loc[session_set.action_type == '-unknown-', 'action_type'] = np.nan\n",
    "        session_set.loc[session_set.action_detail == '-unknown-', 'action_detail'] = np.nan\n",
    "\n",
    "    session_set.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Create a new column to store important actions about booking or receipt\n",
    "\n",
    "    session_set['book_action'] = ((session_set.action.str.contains('book')) & ~(session_set.action.str.contains('facebook'))).astype(int)\n",
    "    session_set['book_action_type'] = session_set.action_type.str.contains('book').astype(int)\n",
    "    session_set['book_action_detail'] = session_set.action_detail.str.contains('book').astype(int)\n",
    "    \n",
    "    session_set.secs_elapsed = session_set.secs_elapsed.fillna(session_set.secs_elapsed.mean())\n",
    "    session_set['book_action_time'] = session_set.book_action * session_set.secs_elapsed\n",
    "    session_set['book_action_type_time'] = session_set.book_action_type * session_set.secs_elapsed\n",
    "    session_set['book_action_detail_time'] = session_set.book_action_detail * session_set.secs_elapsed\n",
    "    \n",
    "    session_set['message_to_host_focus'] = (session_set.action == 'message_to_host_focus').astype(int) \n",
    "    session_set['message_to_host_change'] = (session_set.action == 'message_to_host_change').astype(int) \n",
    "    session_set['host_cancel'] = (session_set.action == 'host_cancel').astype(int) \n",
    "    \n",
    "    \n",
    "    sessions_group = session_set[['user_id', \n",
    "                                  'book_action', \n",
    "                                   'book_action_type', \n",
    "                                   'book_action_detail', \n",
    "                                   'book_action_time',\n",
    "                                   'book_action_type_time',\n",
    "                                   'book_action_detail_time', \n",
    "                                   'message_to_host_focus',\n",
    "                                   'message_to_host_change', \n",
    "                                   'host_cancel',\n",
    "                                   'secs_elapsed'\n",
    "                                 ]].groupby('user_id').sum()\n",
    "    \n",
    "    \n",
    "    if not keep_all:\n",
    "        df = df.merge(sessions_group, how='inner', right_on= 'user_id', left_on = 'id')\n",
    "    else:\n",
    "        df = df.merge(sessions_group, how='left', right_on='user_id', left_on = 'id')\n",
    "        df['book_action'] = df['book_action'].fillna(0)\n",
    "        df['book_action_type'] = df['book_action_type'].fillna(0)\n",
    "        df['book_action_detail'] = df['book_action_detail'].fillna(0)\n",
    "        df['book_action_time'] = df['book_action_time'].fillna(0)\n",
    "        df['book_action_type_time'] = df['book_action_type_time'].fillna(0)\n",
    "        df['book_action_detail_time'] = df['book_action_detail_time'].fillna(0)\n",
    "        df['message_to_host_focus'] = df['message_to_host_focus'].fillna(0)\n",
    "        df['message_to_host_change'] = df['message_to_host_change'].fillna(0)\n",
    "        df['host_cancel'] = df['host_cancel'].fillna(0)\n",
    "        df['secs_elapsed'] = df['secs_elapsed'].fillna(session_set.secs_elapsed.mean())\n",
    "    \n",
    "    df.drop_duplicates(inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def frequence_visitors(age_bucket, gender, country_destination):\n",
    "    \"\"\" This function takes personal information as inputs \n",
    "    like age_bucket and gender of a given person,\n",
    "    and the country_destination we want to suggest her, \n",
    "    and returns the probabilty that this person goes to this country.\"\"\"\n",
    "    num = int(age_gend.population_in_thousands[(age_gend.age_bucket==age_bucket)&(age_gend.gender==gender)&(age_gend.country_destination==country_destination)])\n",
    "    denom = int(age_gend[(age_gend.age_bucket==age_bucket)&(age_gend.gender==gender)].population_in_thousands.sum())\n",
    "    frequence = num/denom\n",
    "    return frequence\n",
    "\n",
    "\n",
    "def get_age_bucket(age):\n",
    "\n",
    "    if age >= 100:\n",
    "        return '100+'\n",
    "    for age_bucket in age_gend.age_bucket.unique():\n",
    "        if age_bucket != '100+':\n",
    "            min_age, max_age = age_bucket.split('-')\n",
    "            min_age, max_age = int(min_age), int(max_age)\n",
    "            if min_age <= age <= max_age:\n",
    "                return age_bucket\n",
    "\n",
    "            \n",
    "def add_visitors(df, dataset_type, dict_destinations_frequency, age_mean=None):\n",
    "    \n",
    "    # convert age to float\n",
    "    df.age = df.age.fillna(0)\n",
    "    df.age = df.age.astype(int) \n",
    "    \n",
    "    # mark non-sense values in age as np.nan\n",
    "    df.loc[(df['age'] < 14), 'age'] = np.nan\n",
    "    df.loc[(df['age'] > 100), 'age'] = np.nan \n",
    "    \n",
    "    # fill missing values in age with mean of age (fit on train)\n",
    "    if dataset_type == 'train':\n",
    "        df.age = df.age.fillna(df.age.mean())\n",
    "    else:\n",
    "        df.age = df.age.fillna(age_mean)\n",
    "    \n",
    "    \n",
    "    df['binary_gender'] = df.gender.str.lower()\n",
    "    df['binary_gender'] = df.binary_gender.apply(\n",
    "        lambda x: np.random.choice(['male', 'female']) if x in ['-unknown-', 'other'] else x)\n",
    "    \n",
    "    df['age_bucket'] = df.age.apply(get_age_bucket)\n",
    "    \n",
    "    for country in age_gend.country_destination.unique():\n",
    "        df['key'] = df.age_bucket + ' ' + df.binary_gender + ' ' + country\n",
    "        df['visitors_2015_'+country] = df.key.map(dict_destinations_frequency)\n",
    "    \n",
    "    df.drop(['key', 'age_bucket', 'binary_gender'], axis=1, inplace=True)\n",
    "                                          \n",
    "    return df \n",
    "   \n",
    "    \n",
    "def draw_graph(df, col, inp):\n",
    "    \"\"\" This function draws a graph with data of a specific series in a dataset (col) about a specific element.\"\"\"\n",
    "    what_to_draw = df[df[col] == inp].country_destination\n",
    "    f, ax = plt.subplots(figsize=(15, 6))\n",
    "    sns.countplot(x=what_to_draw, palette=\"ch:.25\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(col+': '+str(inp))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/amelievogel/Desktop/data_science/Kaggle-Datasets/airbnb-recruiting-new-user-bookings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train set\n",
    "train = pd.read_csv('train_users_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test set\n",
    "test = pd.read_csv('test_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the age-gender set\n",
    "age_gend = pd.read_csv('age_gender_bkts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sessions set\n",
    "sess = pd.read_csv('sessions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and sessions with new features\n",
    "train_2 = merge_sessions(train, sess, keep_all=False)\n",
    "# Merge test and sessions with new features\n",
    "test_2 = merge_sessions(test, sess, keep_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionnary with age_bucket, gender and country to be suggested as keys, and the probability to go to this country as values\n",
    "dict_destinations_frequency = {}\n",
    "for age_bucket in list(age_gend.age_bucket.unique()):\n",
    "    for gend in list(age_gend.gender.unique()):\n",
    "        for country in list(age_gend.country_destination.unique()):\n",
    "            key = ' '.join([age_bucket, gend, country])\n",
    "            value = frequence_visitors(age_bucket, gend, country)\n",
    "            dict_destinations_frequency[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the dict to train and test set so that you have new columns that suggest destination according to age and gender of users\n",
    "train_2 = add_visitors(train_2, 'train', dict_destinations_frequency, age_mean=None)\n",
    "test_2 = add_visitors(test_2, 'test', dict_destinations_frequency, age_mean=int(train.age.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the train set\n",
    "train, col_id_train = preprocessing(train_2, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the test set\n",
    "age_mean = train.age.mean()\n",
    "test, col_id_test = preprocessing(test_2, 'test', age_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now dealing with test_df columns\n",
    "# test_df columns are considered either \"superfluous\" or \"missing\" (in comparison to train_df columns)\n",
    "superfluous_columns = set(test.columns) - set(train.columns)\n",
    "test.drop(superfluous_columns, axis=1, inplace=True)\n",
    "\n",
    "missing_columns = set(train.columns) - set(test.columns) - set(['country_destination'])\n",
    "\n",
    "for column in missing_columns:\n",
    "    test[column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X_train and y_train into np.arrays\n",
    "X, y = train.values[:, :-1], train.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X_train and y_train into np.arrays\n",
    "X_test = test.values[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = pd.read_csv('train_users_2.csv')\n",
    "test_orig = pd.read_csv('test_users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'signup_method', 'google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'is_action_about_booking', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'is_action_about_booking', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'is_action_about_booking', 'Don\\'t know')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"gender\", kind=\"count\", palette=\"ch:.25\", data=train)\n",
    "#plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"age\", palette=\"ch:.25\", data=train)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "sns.countplot(x=\"country_destination\", hue='signup_method', palette=\"ch:.25\", data=train)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'language', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'language', 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'language', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'language', 'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'signup_method', 'facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'signup_method', 'basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'gender', 'female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'gender', 'male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(train_2, 'first_browser', 'Chrome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-validation set\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=876675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=14, n_estimators=200)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=RandomForestClassifier(n_estimators=200, max_depth=14)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#acc_rf=accuracy_score(y_train,y_pred)\n",
    "#pr_rf=precision_score(y_train,y_pred, average = 'weighted')\n",
    "#rec_rf=recall_score(y_train,y_pred, average= 'weighted')\n",
    "#f1_rf=f1_score(y_train,y_pred, average= 'weighted')\n",
    "#print('Acc: %s, prec: %s, rec: %s, f1: %s' % (acc_rf, pr_rf, rec_rf, f1_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.8566583222688717\n"
     ]
    }
   ],
   "source": [
    "# Test on the cross-validation set\n",
    "y_pred=classifier.predict_proba(X_cv)\n",
    "\n",
    "y_cv = pd.get_dummies(y_cv)\n",
    "ndcg_rf = ndcg_score(y_cv, y_pred)\n",
    "print('NDCG: %s' % (ndcg_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier with Balanced Mode with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try feature selection with RFE\n",
    "classifier=RandomForestClassifier()\n",
    "selector_classifier = RFE(classifier)\n",
    "selector_classifier = selector_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=selector_classifier.predict_proba(X_cv)\n",
    "\n",
    "y_cv = pd.get_dummies(y_cv)\n",
    "ndcg_rf = ndcg_score(y_cv, y_pred)\n",
    "print('NDCG: %s' % (ndcg_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest in GridSearch without RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=RandomForestClassifier()\n",
    "\n",
    "# Parameters to test in GridSearch\n",
    "param = {'n_estimators': [50, 100, 200, 300, 400], 'max_depth': range(2, 50, 2)}\n",
    "\n",
    "# Score used to evaluate the model\n",
    "#ndcg_rf = ndcg_score(pd.get_dummies(y_cv), selector_classifier.predict_proba(X_cv))\n",
    "\n",
    "# GridSearch\n",
    "grid = GridSearchCV(estimator=classifier, param_grid=param, scoring='f1_weighted', verbose=10)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_id_test = list(col_id_test)\n",
    "list_index = list(np.argsort(y_pred[0])[::-1])[:5]\n",
    "list_best_countries = []\n",
    "for i in range(len(y_pred)):\n",
    "    list_index = list(np.argsort(y_pred[i])[::-1])[:5]\n",
    "    list_best_countries.append(list(classifier.classes_[list_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ids = [[item] * 5 for item in col_id_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_best_countries = flatten(list_best_countries)\n",
    "list_ids = [[item] * 5 for item in col_id_test]\n",
    "list_ids = flatten(list_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70980</th>\n",
       "      <td>49kpri859i</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70981</th>\n",
       "      <td>49kpri859i</td>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70982</th>\n",
       "      <td>49kpri859i</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70983</th>\n",
       "      <td>49kpri859i</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70984</th>\n",
       "      <td>49kpri859i</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70985 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id country\n",
       "0      5uwns89zht     NDF\n",
       "1      5uwns89zht      US\n",
       "2      5uwns89zht   other\n",
       "3      5uwns89zht      FR\n",
       "4      5uwns89zht      IT\n",
       "...           ...     ...\n",
       "70980  49kpri859i      US\n",
       "70981  49kpri859i     NDF\n",
       "70982  49kpri859i   other\n",
       "70983  49kpri859i      FR\n",
       "70984  49kpri859i      IT\n",
       "\n",
       "[70985 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(zip(list_ids, list_best_countries), columns=['id', 'country'])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(dict_best_countries.items(), columns=['id', 'country']) \n",
    "final_df = final_df.explode('country')\n",
    "final_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission_NDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
